{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yas0HgJyqxw_"
      },
      "source": [
        "# Homework 8: Deep Learning with PyTorch\n",
        "\n",
        "## Dataset\n",
        "\n",
        "In this homework, we'll build a model for classifying various hair types.\n",
        "For this, we will use the Hair Type dataset that was obtained from [Kaggle](https://www.kaggle.com/datasets/kavyasreeb/hair-type-dataset) and slightly rebuilt.\n",
        "\n",
        "You can download the target dataset for this homework from [here](https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip)."
      ],
      "id": "Yas0HgJyqxw_"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S0SLeecgqxxA"
      },
      "outputs": [],
      "source": [
        "# Install specific PyTorch version as per instructions (if needed)\n",
        "# !pip install torch==2.8.0 torchvision"
      ],
      "id": "S0SLeecgqxxA"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sInoPqSqxxB",
        "outputId": "b5bffdb5-3886-4690-da11-c9cac64b33d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-24 11:44:42--  https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/405934815/e712cf72-f851-44e0-9c05-e711624af985?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-11-24T12%3A36%3A32Z&rscd=attachment%3B+filename%3Ddata.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-11-24T11%3A36%3A27Z&ske=2025-11-24T12%3A36%3A32Z&sks=b&skv=2018-11-09&sig=nVgdGMTZf3guJ0zDuzM73EWvZ5ARm5SXQwCMPn4AvEw%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2Mzk4NjQ4MiwibmJmIjoxNzYzOTg0NjgyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.m744ZAfMs40P5KFd2J4HF60MGO9iIasNU_ygHuPJByk&response-content-disposition=attachment%3B%20filename%3Ddata.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-11-24 11:44:42--  https://release-assets.githubusercontent.com/github-production-release-asset/405934815/e712cf72-f851-44e0-9c05-e711624af985?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-11-24T12%3A36%3A32Z&rscd=attachment%3B+filename%3Ddata.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-11-24T11%3A36%3A27Z&ske=2025-11-24T12%3A36%3A32Z&sks=b&skv=2018-11-09&sig=nVgdGMTZf3guJ0zDuzM73EWvZ5ARm5SXQwCMPn4AvEw%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2Mzk4NjQ4MiwibmJmIjoxNzYzOTg0NjgyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.m744ZAfMs40P5KFd2J4HF60MGO9iIasNU_ygHuPJByk&response-content-disposition=attachment%3B%20filename%3Ddata.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 102516572 (98M) [application/octet-stream]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]  97.77M  35.5MB/s    in 2.8s    \n",
            "\n",
            "2025-11-24 11:44:45 (35.5 MB/s) - ‘data.zip’ saved [102516572/102516572]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Download and unzip data if not exists (Colab-ready)\n",
        "if not os.path.exists(\"data.zip\"):\n",
        "    !wget https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip\n",
        "    !unzip -q data.zip"
      ],
      "id": "5sInoPqSqxxB"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uang-4WqqxxB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "import os\n",
        "import statistics\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "id": "uang-4WqqxxB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCydiqQFqxxC"
      },
      "source": [
        "## Reproducibility"
      ],
      "id": "RCydiqQFqxxC"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8nnc6BeCqxxC"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "id": "8nnc6BeCqxxC"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGTQkf7wqxxC",
        "outputId": "f0129b53-6859-4c21-c226-6d89a618fb39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.9.0+cu126\n"
          ]
        }
      ],
      "source": [
        "# Check PyTorch version\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "# Note: The homework asks for version 2.8.0, but we will use the installed version."
      ],
      "id": "VGTQkf7wqxxC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08XtW_hmqxxD"
      },
      "source": [
        "## Model\n",
        "\n",
        "For this homework we will use Convolutional Neural Network (CNN). We'll use PyTorch.\n",
        "\n",
        "Structure:\n",
        "* Input: `(3, 200, 200)`\n",
        "* Conv2d: 32 filters, kernel (3,3), relu\n",
        "* MaxPool2d: (2,2)\n",
        "* Flatten\n",
        "* Linear: 64 neurons, relu\n",
        "* Linear: 1 neuron"
      ],
      "id": "08XtW_hmqxxD"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "86fIlTPCqxxD"
      },
      "outputs": [],
      "source": [
        "class HairClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HairClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
        "        # Calculate input size for linear layer\n",
        "        # Input: 200x200\n",
        "        # Conv (3x3): 198x198\n",
        "        # MaxPool (2x2): 99x99\n",
        "        self.fc1 = nn.Linear(32 * 99 * 99, 64)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "id": "86fIlTPCqxxD"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8m3k0QeqxxD",
        "outputId": "0189a111-fe57-4dae-edaa-2974b1511a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "model = HairClassifier()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "print(f\"Using device: {device}\")"
      ],
      "id": "z8m3k0QeqxxD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cL8pt7ZqxxD"
      },
      "source": [
        "### Question 1\n",
        "Which loss function you will use?"
      ],
      "id": "1cL8pt7ZqxxD"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "10BV9Nb8qxxD"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "# We use BCEWithLogitsLoss because it combines a Sigmoid layer and the BCELoss in one single class.\n",
        "# This is more numerically stable than using a plain Sigmoid followed by a BCELoss."
      ],
      "id": "10BV9Nb8qxxD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVHDgx9WqxxE"
      },
      "source": [
        "### Question 2\n",
        "What's the total number of parameters of the model?"
      ],
      "id": "BVHDgx9WqxxE"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uzs-hnKqxxE",
        "outputId": "fb598dee-56f5-4a46-b184-4519b370eefa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 20073473\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params}\")"
      ],
      "id": "6uzs-hnKqxxE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9S759SpqxxE"
      },
      "source": [
        "### Generators and Training"
      ],
      "id": "x9S759SpqxxE"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GttQzOVuqxxE"
      },
      "outputs": [],
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "# Assuming data is unziped in current directory or 'data' directory\n",
        "base_dir = '.'\n",
        "if os.path.exists('data/train'):\n",
        "    base_dir = 'data'\n",
        "elif os.path.exists('train'):\n",
        "    base_dir = '.'\n",
        "else:\n",
        "    # Fallback if unzipped structure is different or not found yet\n",
        "    # The download cell above should handle this in Colab\n",
        "    print(\"Warning: Data directory not found. Please ensure data is downloaded and unzipped.\")\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "if os.path.exists(train_dir):\n",
        "    train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
        "    val_dataset = datasets.ImageFolder(test_dir, transform=test_transforms)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True, num_workers=2)\n",
        "    validation_loader = DataLoader(val_dataset, batch_size=20, shuffle=False, num_workers=2)\n",
        "else:\n",
        "    print(\"Train directory not found!\")"
      ],
      "id": "GttQzOVuqxxE"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "89MobBS8qxxE"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)"
      ],
      "id": "89MobBS8qxxE"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "p6Uhoy7XqxxF"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=10, start_epoch=0):\n",
        "    history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
        "\n",
        "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            labels = labels.float().unsqueeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = correct_train / total_train\n",
        "        history['loss'].append(epoch_loss)\n",
        "        history['acc'].append(epoch_acc)\n",
        "\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                labels = labels.float().unsqueeze(1)\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_running_loss += loss.item() * images.size(0)\n",
        "                predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
        "        val_epoch_acc = correct_val / total_val\n",
        "        history['val_loss'].append(val_epoch_loss)\n",
        "        history['val_acc'].append(val_epoch_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, \"\n",
        "              f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
        "              f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")\n",
        "\n",
        "    return history"
      ],
      "id": "p6Uhoy7XqxxF"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCATc6YBqxxF",
        "outputId": "53ac25fc-0a3c-4e34-efb7-e426f97ac22c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6462, Acc: 0.6362, Val Loss: 0.6032, Val Acc: 0.6517\n",
            "Epoch 2, Loss: 0.5475, Acc: 0.7100, Val Loss: 0.7251, Val Acc: 0.6318\n",
            "Epoch 3, Loss: 0.5533, Acc: 0.7250, Val Loss: 0.5991, Val Acc: 0.6716\n",
            "Epoch 4, Loss: 0.4802, Acc: 0.7712, Val Loss: 0.6033, Val Acc: 0.6567\n",
            "Epoch 5, Loss: 0.4334, Acc: 0.8025, Val Loss: 0.6196, Val Acc: 0.6766\n",
            "Epoch 6, Loss: 0.3740, Acc: 0.8325, Val Loss: 0.7371, Val Acc: 0.6766\n",
            "Epoch 7, Loss: 0.2721, Acc: 0.8838, Val Loss: 0.9223, Val Acc: 0.6418\n",
            "Epoch 8, Loss: 0.2478, Acc: 0.9000, Val Loss: 0.7294, Val Acc: 0.7214\n",
            "Epoch 9, Loss: 0.2075, Acc: 0.9200, Val Loss: 0.7523, Val Acc: 0.7015\n",
            "Epoch 10, Loss: 0.1494, Acc: 0.9450, Val Loss: 0.7894, Val Acc: 0.7015\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(train_dir):\n",
        "    history = train_model(model, train_loader, validation_loader, optimizer, criterion, num_epochs=10)"
      ],
      "id": "eCATc6YBqxxF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r19pvfx_qxxF"
      },
      "source": [
        "### Question 3\n",
        "What is the median of training accuracy for all the epochs for this model?"
      ],
      "id": "r19pvfx_qxxF"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_n4H7oRxqxxF",
        "outputId": "89ea678b-cc1a-4455-9b34-d073d6d963c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Median training accuracy: 0.8175\n"
          ]
        }
      ],
      "source": [
        "if 'history' in locals():\n",
        "    median_train_acc = statistics.median(history['acc'])\n",
        "    print(f\"Median training accuracy: {median_train_acc:.4f}\")"
      ],
      "id": "_n4H7oRxqxxF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_FWWqT5qxxF"
      },
      "source": [
        "### Question 4\n",
        "What is the standard deviation of training loss for all the epochs for this model?"
      ],
      "id": "0_FWWqT5qxxF"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwxS4tClqxxF",
        "outputId": "15121d32-a6c4-4232-871c-90235c1d620e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard deviation of training loss: 0.1676\n"
          ]
        }
      ],
      "source": [
        "if 'history' in locals():\n",
        "    std_train_loss = statistics.stdev(history['loss'])\n",
        "    print(f\"Standard deviation of training loss: {std_train_loss:.4f}\")"
      ],
      "id": "GwxS4tClqxxF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRdKwb9SqxxG"
      },
      "source": [
        "### Data Augmentation"
      ],
      "id": "bRdKwb9SqxxG"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FwuFrKakqxxG"
      },
      "outputs": [],
      "source": [
        "train_aug_transform = transforms.Compose([\n",
        "    transforms.RandomRotation(50),\n",
        "    transforms.RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "if os.path.exists(train_dir):\n",
        "    train_dataset_aug = datasets.ImageFolder(train_dir, transform=train_aug_transform)\n",
        "    train_loader_aug = DataLoader(train_dataset_aug, batch_size=20, shuffle=True, num_workers=2)"
      ],
      "id": "FwuFrKakqxxG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq4cTe3pqxxG"
      },
      "source": [
        "### Question 5 & 6\n",
        "Train for 10 more epochs."
      ],
      "id": "iq4cTe3pqxxG"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1E3CtYmUqxxG",
        "outputId": "31271c87-b157-42c9-f68c-299d69bf592b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Loss: 0.7217, Acc: 0.6188, Val Loss: 0.5554, Val Acc: 0.7015\n",
            "Epoch 12, Loss: 0.6034, Acc: 0.6850, Val Loss: 0.5599, Val Acc: 0.7214\n",
            "Epoch 13, Loss: 0.5398, Acc: 0.7238, Val Loss: 0.5172, Val Acc: 0.7114\n",
            "Epoch 14, Loss: 0.5217, Acc: 0.7350, Val Loss: 0.5991, Val Acc: 0.7065\n",
            "Epoch 15, Loss: 0.5081, Acc: 0.7675, Val Loss: 0.5517, Val Acc: 0.7164\n",
            "Epoch 16, Loss: 0.4859, Acc: 0.7575, Val Loss: 0.6278, Val Acc: 0.6866\n",
            "Epoch 17, Loss: 0.4668, Acc: 0.7762, Val Loss: 0.6632, Val Acc: 0.6667\n",
            "Epoch 18, Loss: 0.4860, Acc: 0.7738, Val Loss: 0.5240, Val Acc: 0.7562\n",
            "Epoch 19, Loss: 0.4679, Acc: 0.7762, Val Loss: 0.6249, Val Acc: 0.6965\n",
            "Epoch 20, Loss: 0.4569, Acc: 0.7812, Val Loss: 0.6747, Val Acc: 0.6766\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(train_dir) and 'history' in locals():\n",
        "    history_aug = train_model(model, train_loader_aug, validation_loader, optimizer, criterion, num_epochs=10, start_epoch=10)"
      ],
      "id": "1E3CtYmUqxxG"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EKu7RvNqxxG",
        "outputId": "da2e5b57-4a26-4109-8afa-325ebc24b9cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of test loss (augmented training): 0.5898\n"
          ]
        }
      ],
      "source": [
        "if 'history_aug' in locals():\n",
        "    mean_test_loss_aug = statistics.mean(history_aug['val_loss'])\n",
        "    print(f\"Mean of test loss (augmented training): {mean_test_loss_aug:.4f}\")"
      ],
      "id": "2EKu7RvNqxxG"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bZYYMfoqxxG",
        "outputId": "22643380-6e63-4229-f1b9-dffe91af3276"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average test accuracy for last 5 epochs (augmented training): 0.6965\n"
          ]
        }
      ],
      "source": [
        "if 'history_aug' in locals():\n",
        "    mean_test_acc_last5 = statistics.mean(history_aug['val_acc'][5:])\n",
        "    print(f\"Average test accuracy for last 5 epochs (augmented training): {mean_test_acc_last5:.4f}\")"
      ],
      "id": "4bZYYMfoqxxG"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('car_fuel_efficiency.csv')\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n",
    "for c in categorical_columns:\n",
    "    df[c] = df[c].str.lower().str.replace(' ', '_')\n",
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: What is the mode of the `origin` variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'europe'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['origin'].mode()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: What's the median of the `horsepower` variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(146.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['horsepower'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['above_average'] = (df['fuel_efficiency_mpg'] > df['fuel_efficiency_mpg'].mean()).astype(int)\n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "y_train = df_train.above_average.values\n",
    "y_val = df_val.above_average.values\n",
    "y_test = df_test.above_average.values\n",
    "del df_train['above_average']\n",
    "del df_val['above_average']\n",
    "del df_test['above_average']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: What's the accuracy on the validation dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9366306027820711"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical = ['engine_displacement', 'num_cylinders', 'horsepower', 'vehicle_weight', 'acceleration', 'model_year']\n",
    "dv = DictVectorizer(sparse=False)\n",
    "train_dict = df_train[numerical].to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "val_dict = df_val[numerical].to_dict(orient='records')\n",
    "X_val = dv.transform(val_dict)\n",
    "y_pred = model.predict(X_val)\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: What's the least useful feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vehicle_weight'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = numerical\n",
    "results = {}\n",
    "for feature in features:\n",
    "    subset = [f for f in features if f != feature]\n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    train_dict = df_train[subset].to_dict(orient='records')\n",
    "    X_train = dv.fit_transform(train_dict)\n",
    "    model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    val_dict = df_val[subset].to_dict(orient='records')\n",
    "    X_val = dv.transform(val_dict)\n",
    "    y_pred = model.predict(X_val)\n",
    "    results[feature] = accuracy_score(y_val, y_pred)\n",
    "min(results, key=results.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: What's the difference between the sum of misclassifications for the Regularized Logistic Regression model and the Unregularized one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.001: np.int64(1),\n",
       " 0.01: np.int64(0),\n",
       " 0.1: np.int64(0),\n",
       " 1: np.int64(0),\n",
       " 10: np.int64(0)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df['fuel_efficiency_mpg']\n",
    "X = df.drop('above_average', axis=1)\n",
    "y = df['above_average']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "categorical = list(X_train.dtypes[X_train.dtypes == 'object'].index)\n",
    "numerical = list(X_train.dtypes[X_train.dtypes != 'object'].index)\n",
    "dv = DictVectorizer(sparse=False)\n",
    "X_train = dv.fit_transform(X_train.to_dict(orient='records'))\n",
    "X_test = dv.transform(X_test.to_dict(orient='records'))\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "unregularized_misclassifications = (y_pred != y_test).sum()\n",
    "results = {}\n",
    "for C in [0.001, 0.01, 0.1, 1, 10]:\n",
    "    model = LogisticRegression(solver='liblinear', C=C, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    regularized_misclassifications = (y_pred != y_test).sum()\n",
    "    results[C] = unregularized_misclassifications - regularized_misclassifications\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Which of these models has the best RMSE on the validation set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {}\n",
    "for alpha in [0, 0.01, 0.1, 1, 10]:\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    results[alpha] = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "min(results, key=results.get)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-zoomcamp-2025)",
   "language": "python",
   "name": "ml-zoomcamp-2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
